{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3144faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting dataframe setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Causal inference library\n",
    "from dowhy import CausalModel\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#Getting libraries for Logistic Regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "#testing effect modification \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#for modelling \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb7fb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataframe\n",
    "df = pd.read_csv('./out/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a8b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add kill participation \n",
    "df['adc_killpart'] = ((df['adc_kills'] + df['adc_assists']) / df['kills']) \n",
    "df['jng_killpart'] = ((df['jng_kills'] + df['jng_assists']) / df['kills']) \n",
    "\n",
    "#add pre 10 kill participation for jng\n",
    "#df['jng_killpartat10'] = ((df['jng_killsat10']+df['jng_assistsat10']) / df['killsat10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac0a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aefc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define performance features for both roles \n",
    "#mb add adc_totaldamage\n",
    "#for now add only killpart and maybe show trough other stuff that kills also should be add idk\n",
    "#For adc adc_deaths was added looking at TP vs FP\n",
    "#used tp fp analysis with model ['adc_killpart', 'adc_dpm', 'adc_cspm', 'adc_damagetakenperminute','adc_deaths' to add earnedgold -_> 0.77... -> 0.81\n",
    "#remove killpart as it doesnt increase probability\n",
    "adc_performance_features = ['adc_dpm', 'adc_cspm', 'adc_damagetakenperminute','adc_deaths', 'adc_earnedgold']\n",
    "jng_performance_features = ['jng_killpart', 'jng_cspm', 'jng_dpm', 'jng_deaths', 'jng_assists'] \n",
    "#assists and deaths for jungle were added using the distribution analysis of TP and FP while earned gold was added using TN and FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2cf84cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           adc_dpm  adc_cspm  adc_damagetakenperminute  \\\n",
      "adc_dpm                   1.000000  0.145925                  0.298941   \n",
      "adc_cspm                  0.145925  1.000000                 -0.136674   \n",
      "adc_damagetakenperminute  0.298941 -0.136674                  1.000000   \n",
      "adc_deaths               -0.017151 -0.309463                  0.552302   \n",
      "adc_earnedgold            0.550871  0.464974                  0.041858   \n",
      "\n",
      "                          adc_deaths  adc_earnedgold  \n",
      "adc_dpm                    -0.017151        0.550871  \n",
      "adc_cspm                   -0.309463        0.464974  \n",
      "adc_damagetakenperminute    0.552302        0.041858  \n",
      "adc_deaths                  1.000000       -0.197960  \n",
      "adc_earnedgold             -0.197960        1.000000  \n"
     ]
    }
   ],
   "source": [
    "#use pearsoin correlation of variables to avoid double counting in the performance index\n",
    "#use this maybe as defense \n",
    "corr_matrix = df[adc_performance_features].corr(method='pearson')\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930e8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the causal graph\n",
    "#TODO: Draw the proper graph up again please so I can be sure of results\n",
    "causal_graph = '''\n",
    "digraph {\n",
    "    opp_rating_before -> win_prob;\n",
    "    opp_rating_before -> rating_after;\n",
    "    rating_before -> win_prob;\n",
    "    rating_before -> rating_after;\n",
    "    side_adv -> win_prob;\n",
    "    win_prob -> result;\n",
    "    win_prob -> golddiffat15;\n",
    "    win_prob -> gamelength;\n",
    "    visionscore -> gamelength;\n",
    "    visionscore -> kills;\n",
    "    kills -> golddiffat15;\n",
    "    kills -> jng_killpart;\n",
    "    kills -> adc_killpart;\n",
    "    golddiffat15 -> result;\n",
    "    adc_dpm -> adc_kills;\n",
    "    adc_dpm -> adc_assists;\n",
    "    adc_dpm -> result;\n",
    "    jng_dpm -> jng_kills;\n",
    "    jng_dpm -> jng_assists;\n",
    "    adc_kills -> kills;\n",
    "    adc_kills -> adc_killpart;\n",
    "    adc_assists -> adc_killpart;\n",
    "    jng_kills -> kills; \n",
    "    jng_kills -> jng_killpart;\n",
    "    jng_assists -> jng_killpart;\n",
    "    adc_killpart -> result;\n",
    "    jng_killpart -> result;\n",
    "    result -> rating_after;\n",
    "    adc_cspm -> golddiffat15;\n",
    "    adc_cspm -> result;\n",
    "    jng_cspm -> golddiffat15;\n",
    "    jng_cspm -> result;\n",
    "    adc_deaths -> result;\n",
    "    jng_deaths -> result;\n",
    "    adc_damagetakenperminute -> adc_deaths;\n",
    "}\n",
    "'''\n",
    "\n",
    "causal_graph_perf_ind = '''\n",
    "digraph {\n",
    "    opp_rating_before -> win_prob;\n",
    "    opp_rating_before -> rating_after;\n",
    "    rating_before -> win_prob;\n",
    "    rating_before -> rating_after;\n",
    "    side_adv -> win_prob;\n",
    "    win_prob -> result;\n",
    "    win_prob -> golddiffat15;\n",
    "    win_prob -> gamelength;\n",
    "    visionscore -> gamelength;\n",
    "    visionscore -> kills;\n",
    "    kills -> golddiffat15;\n",
    "    kills -> jng_killpart;\n",
    "    kills -> adc_killpart;\n",
    "    golddiffat15 -> result;\n",
    "    adc_dpm -> adc_kills;\n",
    "    adc_dpm -> adc_assists;\n",
    "    jng_dpm -> jng_kills;\n",
    "    jng_dpm -> jng_assists;\n",
    "    adc_kills -> kills;\n",
    "    adc_kills -> adc_killpart;\n",
    "    adc_assists -> adc_killpart;\n",
    "    jng_kills -> kills; \n",
    "    jng_kills -> jng_killpart;\n",
    "    jng_assists -> jng_killpart;\n",
    "    result -> rating_after;\n",
    "    adc_cspm -> golddiffat15;\n",
    "    jng_cspm -> golddiffat15;\n",
    "    adc_damagetakenperminute -> adc_deaths;\n",
    "\n",
    "    opp_rating_before -> adc_performance_index_std;\n",
    "    rating_before -> adc_performance_index_std;\n",
    "    win_prob -> adc_performance_index_std;\n",
    "    side_adv -> adc_performance_index_std;\n",
    "\n",
    "    opp_rating_before -> jng_performance_index_std;\n",
    "    rating_before -> jng_performance_index_std;\n",
    "    win_prob -> jng_performance_index_std;\n",
    "    side_adv -> jng_performance_index_std;\n",
    "\n",
    "    adc_performance_index_std -> adc_kills;\n",
    "    adc_performance_index_std -> adc_assists;\n",
    "    adc_performance_index_std -> adc_deaths;\n",
    "    adc_performance_index_std -> adc_dpm;\n",
    "    adc_performance_index_std -> adc_killpart;\n",
    "\n",
    "    jng_performance_index_std -> jng_kills;\n",
    "    jng_performance_index_std -> jng_assists;\n",
    "    jng_performance_index_std -> jng_deaths;\n",
    "    jng_performance_index_std -> jng_dpm;\n",
    "    jng_performance_index_std -> jng_killpart;\n",
    "\n",
    "    adc_performance_index_std -> result;\n",
    "    jng_performance_index_std -> result;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338c737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4080d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_inference(treatment, outcome, method_name = 'backdoor.linear_regression', data = df):\n",
    "    #Create causal model\n",
    "    model = CausalModel(\n",
    "        data = data,\n",
    "        treatment = treatment,\n",
    "        outcome = outcome,\n",
    "        graph = causal_graph_perf_ind\n",
    "    )\n",
    "\n",
    "    #Identify causal effects\n",
    "    identified_estimand = model.identify_effect()\n",
    "\n",
    "    #Estimate the causal effect using backdoor adjustment with linearregression\n",
    "    estimate = model.estimate_effect(\n",
    "        identified_estimand,\n",
    "        method_name = method_name\n",
    "    )\n",
    "    \n",
    "    refute_results = None\n",
    "    #refute_results = model.refute_estimate(identified_estimand, estimate, method_name='random_common_cause')\n",
    "\n",
    "    return estimate, refute_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33bdf376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for feature in adc_performance_features:\\n    estimate, _ = causal_inference(feature, 'result')\\n    print(f'Estimate for the causal effect of {feature} on the result: {estimate.value}')\\n    \\nfor feature in jng_performance_features:\\n    estimate, _ = causal_inference(feature, 'result')\\n    print(f'Estimate for the causal effect of {feature} on the result: {estimate.value}')\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for feature in adc_performance_features:\n",
    "    estimate, _ = causal_inference(feature, 'result')\n",
    "    print(f'Estimate for the causal effect of {feature} on the result: {estimate.value}')\n",
    "    \n",
    "for feature in jng_performance_features:\n",
    "    estimate, _ = causal_inference(feature, 'result')\n",
    "    print(f'Estimate for the causal effect of {feature} on the result: {estimate.value}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a01623",
   "metadata": {},
   "source": [
    "Estimate for the causal effect of adc_killpart on the result: 0.1600550424721764\n",
    "Estimate for the causal effect of adc_dpm on the result: 0.0002592780663185912\n",
    "Estimate for the causal effect of adc_cspm on the result: 0.006768097456448019\n",
    "Estimate for the causal effect of adc_damagetakenperminute on the result: -0.0007198617017317721\n",
    "Estimate for the causal effect of adc_deaths on the result: -0.1134743559687198\n",
    "Estimate for the causal effect of jng_killpart on the result: 0.005632731760707443\n",
    "Estimate for the causal effect of jng_cspm on the result: 0.016706927469950283\n",
    "Estimate for the causal effect of jng_dpm on the result: 0.0003182545692026473\n",
    "Estimate for the causal effect of jng_deaths on the result: -0.10252868545166982\n",
    "Estimate for the causal effect of jng_assists on the result: 0.03617036774012511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ffb184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8110632183908046\n",
      "AUC: 0.8804574733194628\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.80      0.81     20772\n",
      "           1       0.80      0.83      0.81     20988\n",
      "\n",
      "    accuracy                           0.81     41760\n",
      "   macro avg       0.81      0.81      0.81     41760\n",
      "weighted avg       0.81      0.81      0.81     41760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#We want to add a model for each set of performance variables which tries to predict outcomes for both games\n",
    "#Model using the adc performance features\n",
    "X_adc = df[adc_performance_features]\n",
    "y = df['result']    #same for adc and jng\n",
    "\n",
    "#Create the train and test split with size of test set being 0.3\n",
    "X_adc_train, X_adc_test, y_adc_train, y_adc_test = train_test_split(X_adc, y, random_state=42, test_size=0.3)\n",
    "\n",
    "#Scale data\n",
    "scaler = StandardScaler()\n",
    "X_adc_train_scaled = scaler.fit_transform(X_adc_train)\n",
    "X_adc_test_scaled = scaler.fit_transform(X_adc_test)\n",
    "\n",
    "log_reg_adc = LogisticRegression()\n",
    "#Train model on the training data\n",
    "log_reg_adc.fit(X_adc_train_scaled, y_adc_train)\n",
    "\n",
    "#Predict using the testing data\n",
    "y_adc_pred = log_reg_adc.predict(X_adc_test_scaled)\n",
    "y_adc_prob = log_reg_adc.predict_proba(X_adc_test_scaled)[:,1]\n",
    "\n",
    "#Print results\n",
    "print(f'Accuracy:{accuracy_score(y_adc_test, y_adc_pred)}')\n",
    "print(\"AUC:\", roc_auc_score(y_adc_test, y_adc_prob))\n",
    "print(\"Classification Report:\\n\", classification_report(y_adc_test, y_adc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebabaacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Coefficients\n",
      "\n",
      "Term                                  Value\n",
      "---------------------------------------------\n",
      "Intercept (β₀)                    -0.070335\n",
      "adc_dpm                            0.337668\n",
      "adc_cspm                          -0.704063\n",
      "adc_damagetakenperminute          -0.165350\n",
      "adc_deaths                        -1.523100\n",
      "adc_earnedgold                     1.288004\n"
     ]
    }
   ],
   "source": [
    "#define function to print out coefficient matrix of a give logistic regression model\n",
    "def log_reg_coefficient_summary(log_reg, features):\n",
    "    coef_matrix = log_reg.coef_\n",
    "    constant_coef = log_reg.intercept_[0]\n",
    "    print(\"\\nLogistic Regression Coefficients\\n\")\n",
    "\n",
    "    print(f\"{'Term':<30} {'Value':>12}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    print(f\"{'Intercept (β₀)':<30} {constant_coef:>12.6f}\")\n",
    "\n",
    "    for name, coef in zip(features, coef_matrix[0]):\n",
    "        print(f\"{name:<30} {coef:>12.6f}\")\n",
    "\n",
    "log_reg_coefficient_summary(log_reg_adc, adc_performance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a9aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a results dataframe \n",
    "full_test_adc = df.loc[X_adc_test.index].copy()\n",
    "\n",
    "# Add the model predictions\n",
    "full_test_adc['predicted_result_adc'] = y_adc_pred\n",
    "full_test_adc['predicted_prob_adc'] = y_adc_prob\n",
    "\n",
    "#create tp, fp, tn, fn\n",
    "tp_adc = full_test_adc[\n",
    "    (full_test_adc['result'] == 1) &\n",
    "    (full_test_adc['predicted_result_adc'] == 1)\n",
    "]\n",
    "\n",
    "fp_adc = full_test_adc[\n",
    "    (full_test_adc['result'] == 0) &\n",
    "    (full_test_adc['predicted_result_adc'] == 1)\n",
    "]\n",
    "\n",
    "tn_adc = full_test_adc[\n",
    "    (full_test_adc['result'] == 0) &\n",
    "    (full_test_adc['predicted_result_adc'] == 0)\n",
    "]\n",
    "\n",
    "fn_adc = full_test_adc[\n",
    "    (full_test_adc['result'] == 1) &\n",
    "    (full_test_adc['predicted_result_adc'] == 0)\n",
    "]\n",
    "\n",
    "#filter for features to look at \n",
    "numeric_features_adc = full_test_adc.select_dtypes(include=['number']).columns\n",
    "other_features_adc = [f for f in numeric_features_adc if f not in adc_performance_features + ['result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d73344f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for feature in other_features_adc:\\n    plt.figure(figsize=(8,4))\\n    sns.kdeplot(tp_adc[feature], label=\"TP\", fill=True)\\n    sns.kdeplot(fp_adc[feature], label=\"FP\", fill=True)\\n    plt.title(f\"Distribution of {feature}\")\\n    plt.legend()\\n    plt.show()\\n\\n    #from this the most noticable is definetely the adc_deaths as a strong indicator increasing accuracy by around 9% \\n    #have closer look at FN and TN again\\n    #also look at the papers '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model other features as distribution of tp vs fp\n",
    "\n",
    "'''for feature in other_features_adc:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.kdeplot(tp_adc[feature], label=\"TP\", fill=True)\n",
    "    sns.kdeplot(fp_adc[feature], label=\"FP\", fill=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    #from this the most noticable is definetely the adc_deaths as a strong indicator increasing accuracy by around 9% \n",
    "    #have closer look at FN and TN again\n",
    "    #also look at the papers '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88696ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for feature in other_features_adc:\\n    plt.figure(figsize=(8,4))\\n    sns.kdeplot(tn_adc[feature], label=\"tn\", fill=True)\\n    sns.kdeplot(fn_adc[feature], label=\"fn\", fill=True)\\n    plt.title(f\"Distribution of {feature}\")\\n    plt.legend()\\n    plt.show()'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now look at the same comparison but between FN and TN\n",
    "'''for feature in other_features_adc:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.kdeplot(tn_adc[feature], label=\"tn\", fill=True)\n",
    "    sns.kdeplot(fn_adc[feature], label=\"fn\", fill=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "979babff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8833333333333333\n",
      "AUC: 0.9501222921034242\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     20772\n",
      "           1       0.88      0.89      0.88     20988\n",
      "\n",
      "    accuracy                           0.88     41760\n",
      "   macro avg       0.88      0.88      0.88     41760\n",
      "weighted avg       0.88      0.88      0.88     41760\n",
      "\n",
      "\n",
      "Logistic Regression Coefficients\n",
      "\n",
      "Term                                  Value\n",
      "---------------------------------------------\n",
      "Intercept (β₀)                     0.006484\n",
      "jng_killpart                      -1.032801\n",
      "jng_cspm                           0.279170\n",
      "jng_dpm                            0.301663\n",
      "jng_deaths                        -2.071417\n",
      "jng_assists                        2.538956\n"
     ]
    }
   ],
   "source": [
    "#Run same logistic regression but with jungle \n",
    "#Model using the jng performance features\n",
    "X_jng = df[jng_performance_features]\n",
    "y = df['result']    #y values are same for jungle and adc\n",
    "\n",
    "#Create the train and test split with size of test set being 0.3\n",
    "X_jng_train, X_jng_test, y_jng_train, y_jng_test = train_test_split(X_jng, y, random_state=42, test_size=0.3)\n",
    "\n",
    "#Scale data\n",
    "scaler = StandardScaler()\n",
    "X_jng_train_scaled = scaler.fit_transform(X_jng_train)\n",
    "X_jng_test_scaled = scaler.fit_transform(X_jng_test)\n",
    "\n",
    "log_reg_jng = LogisticRegression()\n",
    "#Train model on the training data\n",
    "log_reg_jng.fit(X_jng_train_scaled, y_jng_train)\n",
    "\n",
    "#Predict using the testing data\n",
    "y_jng_pred = log_reg_jng.predict(X_jng_test_scaled)\n",
    "y_jng_prob = log_reg_jng.predict_proba(X_jng_test_scaled)[:,1]\n",
    "\n",
    "#Print results\n",
    "print(f'Accuracy:{accuracy_score(y_jng_test, y_jng_pred)}')\n",
    "print(\"AUC:\", roc_auc_score(y_jng_test, y_jng_prob))\n",
    "print(\"Classification Report:\\n\", classification_report(y_jng_test, y_jng_pred))\n",
    "log_reg_coefficient_summary(log_reg_jng, jng_performance_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e964bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a results dataframe \n",
    "full_test_jng = df.loc[X_jng_test.index].copy()\n",
    "\n",
    "# Add the model predictions\n",
    "full_test_jng['predicted_result_jng'] = y_jng_pred\n",
    "full_test_jng['predicted_prob_jng'] = y_jng_prob\n",
    "\n",
    "#create tp, fp, tn, fn\n",
    "tp_jng = full_test_jng[\n",
    "    (full_test_jng['result'] == 1) &\n",
    "    (full_test_jng['predicted_result_jng'] == 1)\n",
    "]\n",
    "\n",
    "fp_jng = full_test_jng[\n",
    "    (full_test_jng['result'] == 0) &\n",
    "    (full_test_jng['predicted_result_jng'] == 1)\n",
    "]\n",
    "\n",
    "tn_jng = full_test_jng[\n",
    "    (full_test_jng['result'] == 0) &\n",
    "    (full_test_jng['predicted_result_jng'] == 0)\n",
    "]\n",
    "\n",
    "fn_jng = full_test_jng[\n",
    "    (full_test_jng['result'] == 1) &\n",
    "    (full_test_jng['predicted_result_jng'] == 0)\n",
    "]\n",
    "\n",
    "#filter for features to look at \n",
    "numeric_features_jng = full_test_jng.select_dtypes(include=['number']).columns\n",
    "other_features_jng = [f for f in numeric_features_jng if f not in jng_performance_features + ['result']+ adc_performance_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "910d582c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for feature in other_features_jng:\\n    plt.figure(figsize=(8,4))\\n    sns.kdeplot(tp_jng[feature], label=\"TP\", fill=True)\\n    sns.kdeplot(fp_jng[feature], label=\"FP\", fill=True)\\n    plt.title(f\"Distribution of {feature}\")\\n    plt.legend()\\n    plt.show()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at what are the major differences by looking at the distributions of correct win pred and wrong win pred \n",
    "'''for feature in other_features_jng:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.kdeplot(tp_jng[feature], label=\"TP\", fill=True)\n",
    "    sns.kdeplot(fp_jng[feature], label=\"FP\", fill=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75ebf5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for feature in other_features_jng:\\n    plt.figure(figsize=(8,4))\\n    sns.kdeplot(tn_jng[feature], label=\"TN\", fill=True)\\n    sns.kdeplot(fn_jng[feature], label=\"FN\", fill=True)\\n    plt.title(f\"Distribution of {feature}\")\\n    plt.legend()\\n    plt.show()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for feature in other_features_jng:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.kdeplot(tn_jng[feature], label=\"TN\", fill=True)\n",
    "    sns.kdeplot(fn_jng[feature], label=\"FN\", fill=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fde5d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.684605\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 result   No. Observations:               139197\n",
      "Model:                          Logit   Df Residuals:                   139195\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 15 Jan 2026   Pseudo R-squ.:                 0.01232\n",
      "Time:                        13:51:43   Log-Likelihood:                -95295.\n",
      "converged:                       True   LL-Null:                       -96483.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=================================================================================\n",
      "                    coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------\n",
      "Intercept        -1.4247      0.030    -46.923      0.000      -1.484      -1.365\n",
      "Q(\"total cs\")     0.0014   2.84e-05     47.831      0.000       0.001       0.001\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_base = smf.logit(\n",
    "    'result ~ Q(\"total cs\")',\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "print(model_base.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d702a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592880\n",
      "         Iterations 5\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 result   No. Observations:               139197\n",
      "Model:                          Logit   Df Residuals:                   139194\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Thu, 15 Jan 2026   Pseudo R-squ.:                  0.1446\n",
      "Time:                        13:51:44   Log-Likelihood:                -82527.\n",
      "converged:                       True   LL-Null:                       -96483.\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "====================================================================================\n",
      "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------\n",
      "Intercept            0.0054      0.006      0.905      0.365      -0.006       0.017\n",
      "adc_golddiffat15     0.0007   6.51e-06    102.770      0.000       0.001       0.001\n",
      "jng_golddiffat15     0.0008   8.01e-06    100.951      0.000       0.001       0.001\n",
      "====================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Intercept           1.005386\n",
       "adc_golddiffat15    1.000670\n",
       "jng_golddiffat15    1.000809\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_adc = smf.logit(\n",
    "    \"result ~ adc_golddiffat15 + jng_golddiffat15\",\n",
    "    data=df\n",
    ").fit()\n",
    "\n",
    "print(model_adc.summary())\n",
    "#kind of implies that cs is stronger performance indicator for jungle than for adc \n",
    "#adc_total_cs    1.000581\n",
    "#jng_total_cs    1.007934\n",
    "#adc_kills    1.497634\n",
    "#jng_kills    1.358577\n",
    "import numpy as np\n",
    "\n",
    "np.exp(model_adc.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d717562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"#Try to create latent variable \\n#using or atleast assuming latent variable model  \\n\\ndef create_perf_ind(features, log_reg: LogisticRegression, name):\\n    scaler = StandardScaler()\\n    Z = scaler.fit_transform(df[features])\\n\\n    #get the weights based on the logistic regression object\\n    weights = log_reg.coef_[0]\\n\\n    df[name] = np.dot(Z,weights)\\n    df[name + '_std'] = (df[name] - df[name].mean()) / df[name].std()\\n\\n#create adc and jng performance index\\ncreate_perf_ind(adc_performance_features, log_reg_adc, 'adc_performance_index')\\ncreate_perf_ind(jng_performance_features, log_reg_jng, 'jng_performance_index')\\n\\nprint(df[['adc_performance_index_std', 'jng_performance_index_std']]) \""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#Try to create latent variable \n",
    "#using or atleast assuming latent variable model  \n",
    "\n",
    "def create_perf_ind(features, log_reg: LogisticRegression, name):\n",
    "    scaler = StandardScaler()\n",
    "    Z = scaler.fit_transform(df[features])\n",
    "\n",
    "    #get the weights based on the logistic regression object\n",
    "    weights = log_reg.coef_[0]\n",
    "\n",
    "    df[name] = np.dot(Z,weights)\n",
    "    df[name + '_std'] = (df[name] - df[name].mean()) / df[name].std()\n",
    "\n",
    "#create adc and jng performance index\n",
    "create_perf_ind(adc_performance_features, log_reg_adc, 'adc_performance_index')\n",
    "create_perf_ind(jng_performance_features, log_reg_jng, 'jng_performance_index')\n",
    "\n",
    "print(df[['adc_performance_index_std', 'jng_performance_index_std']]) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e126264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"estimate, _ = causal_inference('jng_performance_index_std', 'result')\\nprint(f'Estimate for the causal effect of jng_performance_index_std on the result: {estimate.value}')\\n\\nestimate, _ = causal_inference('adc_performance_index_std', 'result')\\nprint(f'Estimate for the causal effect of adc_performance_index_std on the result: {estimate.value}')\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''estimate, _ = causal_inference('jng_performance_index_std', 'result')\n",
    "print(f'Estimate for the causal effect of jng_performance_index_std on the result: {estimate.value}')\n",
    "\n",
    "estimate, _ = causal_inference('adc_performance_index_std', 'result')\n",
    "print(f'Estimate for the causal effect of adc_performance_index_std on the result: {estimate.value}')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2402efb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        -0.273911\n",
      "1        -1.753341\n",
      "2        -0.130052\n",
      "3        -0.623404\n",
      "4        -0.810846\n",
      "            ...   \n",
      "139192    1.708834\n",
      "139193    0.634976\n",
      "139194    1.538290\n",
      "139195    2.164805\n",
      "139196    1.153990\n",
      "Name: ADC_Performance_std, Length: 139197, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from semopy import Model\n",
    "\n",
    "df['adc_deaths_rev'] = -df['adc_deaths']\n",
    "df['adc_damagetakenperminute_rev'] = -df['adc_damagetakenperminute']\n",
    "\n",
    "#Define CFA model\n",
    "model_desc = '''\n",
    "ADC_Performance =~ adc_dpm + adc_cspm + adc_damagetakenperminute_rev + adc_deaths_rev + adc_earnedgold\n",
    "'''\n",
    "\n",
    "#Estimate both the factor loadings for each indicator and also the measurement errors by fitting a model to the data\n",
    "model_adc = Model(model_desc)\n",
    "model_adc.fit(df)\n",
    "\n",
    "#Estimate factor scores using MAP\n",
    "factor_scores = model_adc.predict_factors(df)\n",
    "\n",
    "factor_scores['ADC_Performance_std'] = (\n",
    "    factor_scores['ADC_Performance'] - factor_scores['ADC_Performance'].mean()\n",
    ") / factor_scores['ADC_Performance'].std()\n",
    "\n",
    "print(factor_scores['ADC_Performance_std'])\n",
    "\n",
    "factor_scores = factor_scores.reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['adc_performance_index_std'] = factor_scores['ADC_Performance_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         0.465460\n",
      "1        -0.390623\n",
      "2         1.156382\n",
      "3        -0.152914\n",
      "4        -0.660452\n",
      "            ...   \n",
      "139192    0.289807\n",
      "139193    0.597777\n",
      "139194    0.175794\n",
      "139195   -0.235345\n",
      "139196    0.869370\n",
      "Name: JNG_Performance_std, Length: 139197, dtype: float64\n",
      "               lval  op             rval      Estimate   Std. Err     z-value  \\\n",
      "0      jng_killpart   ~  JNG_Performance      1.000000          -           -   \n",
      "1          jng_cspm   ~  JNG_Performance      1.658021   0.046147    35.92898   \n",
      "2           jng_dpm   ~  JNG_Performance    150.719974   4.545619   33.157196   \n",
      "3        jng_deaths   ~  JNG_Performance     -4.299091   0.083972  -51.196798   \n",
      "4       jng_assists   ~  JNG_Performance     24.060488   0.440356   54.638729   \n",
      "5   JNG_Performance  ~~  JNG_Performance      0.011209   0.000225   49.806033   \n",
      "6       jng_assists  ~~      jng_assists     13.013608   0.128476  101.292358   \n",
      "7          jng_cspm  ~~         jng_cspm      1.470994   0.005684  258.817725   \n",
      "8        jng_deaths  ~~       jng_deaths      3.887980   0.015494  250.936702   \n",
      "9           jng_dpm  ~~          jng_dpm  14679.690363  56.529291  259.682901   \n",
      "10     jng_killpart  ~~     jng_killpart      0.021367    0.00022   97.002738   \n",
      "\n",
      "   p-value  \n",
      "0        -  \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "5      0.0  \n",
      "6      0.0  \n",
      "7      0.0  \n",
      "8      0.0  \n",
      "9      0.0  \n",
      "10     0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#jng_performance_features = ['jng_killpart', 'jng_cspm', 'jng_dpm', 'jng_deaths', 'jng_assists'] \n",
    "\n",
    "df['jng_deaths_rev'] = -df['jng_deaths']\n",
    "\n",
    "#measurement model e.g. CFA model\n",
    "model_desc_jng = '''\n",
    "JNG_Performance =~ jng_killpart + jng_cspm + jng_dpm + jng_deaths + jng_assists\n",
    "'''\n",
    "\n",
    "model_jng = Model(model_desc_jng)\n",
    "model_jng.fit(df)\n",
    "\n",
    "factor_scores = model_jng.predict_factors(df)\n",
    "\n",
    "factor_scores['JNG_Performance_std'] = (\n",
    "    factor_scores['JNG_Performance'] - factor_scores['JNG_Performance'].mean()\n",
    ") / factor_scores['JNG_Performance'].std()\n",
    "\n",
    "print(model_jng.inspect())\n",
    "\n",
    "factor_scores = factor_scores.reset_index(drop=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['jng_performance_index_std'] = factor_scores['JNG_Performance_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57d96de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139197\n",
      "139197\n",
      "gameid                          0\n",
      "teamid                          0\n",
      "result                          0\n",
      "side                            0\n",
      "kills                           0\n",
      "                               ..\n",
      "adc_deaths_rev                  0\n",
      "adc_damagetakenperminute_rev    0\n",
      "adc_performance_index_std       0\n",
      "jng_deaths_rev                  0\n",
      "jng_performance_index_std       0\n",
      "Length: 71, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(factor_scores))\n",
    "print(len(df))\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "37d113fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate for the causal effect of jng_performance_index_std on the result: 0.19494863576654764\n",
      "Estimate for the causal effect of adc_performance_index_std on the result: 0.12346041371413508\n"
     ]
    }
   ],
   "source": [
    "estimate, _ = causal_inference('jng_performance_index_std', 'result', data=df)\n",
    "print(f'Estimate for the causal effect of jng_performance_index_std on the result: {estimate.value}')\n",
    "\n",
    "estimate, _ = causal_inference('adc_performance_index_std', 'result', data=df)\n",
    "print(f'Estimate for the causal effect of adc_performance_index_std on the result: {estimate.value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
